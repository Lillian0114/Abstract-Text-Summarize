{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTOnDocffEx3",
        "outputId": "57c994dd-f324-4d29-f3df-c367e52ed7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (4.64.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers) (3.7)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=911ab5b834439e141195e945d91117f41cd8046715f6ba4e2cb18d6acd9c197d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.12.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install spacy-langdetect\n",
        "# !pip install language-detector\n",
        "# !pip install symspellpy\n",
        "!pip install sentence-transformers\n",
        "# !pip install umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wsruohOHFJ9"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "from gensim import corpora\n",
        "import gensim\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import keras\n",
        "from keras.layers import Input, Bidirectional, LSTM, Dense, RepeatVector, Concatenate, Activation, Lambda, Dot, Softmax, TimeDistributed, Dropout, Layer\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "from nltk.tokenize import word_tokenize\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IG9IzhsivCrs",
        "outputId": "53453842-23ba-4734-c6c3-4ac94938958d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pr3yEMlH4s1",
        "outputId": "ed01b8a1-94fc-44e3-98b8-0a837b7042a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\"\"\" connect google drive \"\"\"\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcYtxBPZ5AyA"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.metrics import silhouette_score\n",
        "# import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMZcfXZw49rH"
      },
      "outputs": [],
      "source": [
        "def get_topic_words(token_lists, labels, k=None):\n",
        "  \"\"\"\n",
        "  get top words within each topic from clustering results\n",
        "  \"\"\"\n",
        "  if k is None:\n",
        "    k = len(np.unique(labels))\n",
        "  topics = ['' for _ in range(k)]\n",
        "  for i, c in enumerate(token_lists):\n",
        "    topics[labels[i]] += (' ' + ' '.join(c))\n",
        "  word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n",
        "  # get sorted word counts\n",
        "  word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n",
        "  # get topics\n",
        "  topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n",
        "\n",
        "  return topics\n",
        "\n",
        "def get_coherence(model, token_lists, measure='c_v'):\n",
        "  \"\"\"\n",
        "  Get model coherence from gensim.models.coherencemodel\n",
        "  :param model: Topic_Model object\n",
        "  :param token_lists: token lists of docs\n",
        "  :param topics: topics as top words\n",
        "  :param measure: coherence metrics\n",
        "  :return: coherence score\n",
        "  \"\"\"\n",
        "  if model.method == 'LDA':\n",
        "    cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus,dictionary=model.dictionary, coherence=measure)\n",
        "  else:\n",
        "    topics = get_topic_words(token_lists, model.cluster_model.labels_)\n",
        "    cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary, coherence=measure)\n",
        "    print(cm.get_coherence_per_topic())\n",
        "  return cm.get_coherence()\n",
        "\n",
        "def get_silhouette(model):\n",
        "  \"\"\"\n",
        "  Get silhouette score from model\n",
        "  :param model: Topic_Model object\n",
        "  :return: silhouette score\n",
        "  \"\"\"\n",
        "  if model.method == 'LDA':\n",
        "    return\n",
        "  lbs = model.cluster_model.labels_\n",
        "  vec = model.vec[model.method]\n",
        "  return silhouette_score(vec, lbs)\n",
        "\n",
        "def get_wordcloud(model, token_lists, topic):\n",
        "  \"\"\"\n",
        "  Get word cloud of each topic from fitted model\n",
        "  :param model: Topic_Model object\n",
        "  :param sentences: preprocessed sentences from docs\n",
        "  \"\"\"\n",
        "  if model.method == 'LDA':\n",
        "    return\n",
        "  print('Getting wordcloud for topic {} ...'.format(topic))\n",
        "  lbs = model.cluster_model.labels_\n",
        "  tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n",
        "\n",
        "  print(tokens)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIeuhovgK7Ml"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfDoIupVIcAj"
      },
      "outputs": [],
      "source": [
        "class Autoencoder:\n",
        "  \"\"\"\n",
        "  Autoencoder for learning latent space representation\n",
        "  architecture simplified for only one hidden layer\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
        "    self.latent_dim = latent_dim\n",
        "    self.activation = activation\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.autoencoder = None\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.his = None\n",
        "\n",
        "  def _compile(self, input_dim):\n",
        "    \"\"\"\n",
        "    compile the computational graph\n",
        "    \"\"\"\n",
        "    input_vec = Input(shape=(input_dim,))\n",
        "    encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
        "    # print(\"encoded: \",encoded)\n",
        "    decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
        "    # print(\"decoded: \",decoded)\n",
        "    self.autoencoder = Model(input_vec, decoded, name='autoencoder')\n",
        "    # print(self.autoencoder.summary())\n",
        "    self.encoder = Model(input_vec, encoded, name='encoder')\n",
        "    # print()\n",
        "    # print(self.encoder.summary())\n",
        "    encoded_input = Input(shape=(self.latent_dim,))\n",
        "    decoder_layer = self.autoencoder.layers[-1]\n",
        "    self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input), name='decoder')\n",
        "    # print()\n",
        "    # print(self.decoder.summary())\n",
        "    self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "\n",
        "  def fit(self, X):\n",
        "    if not self.autoencoder:\n",
        "      print(\"input_dim形狀: \", X.shape)\n",
        "      self._compile(X.shape[1])\n",
        "    X_train, X_test = train_test_split(X)\n",
        "    self.his = self.autoencoder.fit(X_train, X_train, epochs=200,\n",
        "                     batch_size=128, shuffle=True,\n",
        "                     validation_data=(X_test, X_test), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNfxGTRpYoVB"
      },
      "outputs": [],
      "source": [
        "class Sampling(Layer):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Sampling, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    mu, log_var = inputs\n",
        "    epsilon = K.random_normal(shape=(K.shape(mu)[0], self.latent_dim))\n",
        "    return mu + K.exp(0.5 * log_var) * epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EpA7se8PFLF"
      },
      "outputs": [],
      "source": [
        "class NTM_Autoencoder:\n",
        "  def __init__(self, latent_dim=32, intermediate_dim = 96, activation='relu', epochs=200, batch_size=128, num_topics=15, dropout = 0.2):\n",
        "    self.latent_dim = latent_dim\n",
        "    self.intermediate_dim = intermediate_dim\n",
        "    self.activation = activation\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "    self.dropout = dropout\n",
        "    self.autoencoder = None\n",
        "    self.encoder = None\n",
        "    self.decoder = None\n",
        "    self.his = None\n",
        "    # self.num_topics = num_topics\n",
        "\n",
        "  def _compile(self, input_dim):\n",
        "    \"\"\"\n",
        "    compile the computational graph\n",
        "    \"\"\"\n",
        "\n",
        "    input_vec = Input(shape=(input_dim,))\n",
        "    encoded = Dense(self.intermediate_dim, activation=self.activation)(input_vec)\n",
        "    encoded = Dropout(self.dropout)(encoded)\n",
        "    mu = Dense(self.latent_dim)(encoded)\n",
        "    log_var = Dense(self.latent_dim)(encoded)\n",
        "\n",
        "    def sampling(args):\n",
        "      z_mean, z_log_var = args\n",
        "      epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.latent_dim), mean=0., stddev=1.)\n",
        "      return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "    # z = Sampling(self.latent_dim)([mu, log_var])\n",
        "    z = Lambda(sampling)([mu, log_var])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_input = Input(shape=(self.latent_dim,))\n",
        "    x = Dense(self.intermediate_dim, activation=self.activation)(decoder_input)\n",
        "    outputs = Dense(input_dim, activation=self.activation)(x)\n",
        "    \n",
        "    # decoded = Dense(self.latent_dim, activation=self.activation)(z)\n",
        "    # outputs = Dropout(self.dropout)(decoded)\n",
        "    \n",
        "    self.encoder = Model(input_vec, [mu, log_var, z], name='encoder')\n",
        "    # print(self.encoder.summary())\n",
        "    # print()\n",
        "    self.decoder = Model(decoder_input, outputs, name='decoder')\n",
        "    # print(self.decoder.summary())\n",
        "    # print()\n",
        "    outputs = self.decoder(self.encoder(input_vec)[2])\n",
        "    self.autoencoder = Model(input_vec, outputs, name='autoencoder')\n",
        "    # print(self.autoencoder.summary())\n",
        "    # print()\n",
        "      \n",
        "    def vae_loss(inputs, outputs):\n",
        "      reconstruction_loss = K.mean(keras.losses.binary_crossentropy(inputs, outputs))\n",
        "      kl_loss = -0.5 * K.sum(1 + log_var - K.square(mu) - K.exp(log_var))\n",
        "      return K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "    # self.autoencoder.compile(optimizer='adam', loss=vae_loss)\n",
        "    self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
        "  \n",
        "  def predict(self, x_test):\n",
        "    return self.vae.predict(x_test)\n",
        "\n",
        "  def fit(self, X):\n",
        "    if not self.autoencoder:\n",
        "      print(\"input_dim形狀: \", X.shape)\n",
        "      self._compile(X.shape[1])\n",
        "    X_train, X_test = train_test_split(X)\n",
        "    self.his = self.autoencoder.fit(X_train, X_train, epochs=200,\n",
        "                     batch_size=128, shuffle=True,\n",
        "                     validation_data=(X_test, X_test), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epNAC8WFz3KL"
      },
      "outputs": [],
      "source": [
        "# define model object\n",
        "class Topic_Model:\n",
        "  def __init__(self, k=10, method='TFIDF'):\n",
        "    \"\"\"\n",
        "    :param k: number of topics\n",
        "    :param method: method chosen for the topic model\n",
        "    \"\"\"\n",
        "    if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n",
        "      raise Exception('Invalid method!')\n",
        "    self.k = k\n",
        "    self.dictionary = None\n",
        "    self.corpus = None\n",
        "    # self.stopwords = None\n",
        "    self.cluster_model = None\n",
        "    self.ldamodel = None\n",
        "    self.vec = {}\n",
        "    self.gamma = 15  # parameter for reletive importance of lda\n",
        "    self.method = method\n",
        "    self.AE = None\n",
        "    self.id = method + '_' + datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "  def vectorize(self, sentences, token_lists, method=None):\n",
        "    \"\"\"\n",
        "    Get vecotr representations from selected methods\n",
        "    \"\"\"\n",
        "    # Default method\n",
        "    if method is None:\n",
        "      method = self.method\n",
        "\n",
        "    # turn tokenized documents into a id <-> term dictionary\n",
        "    self.dictionary = corpora.Dictionary(token_lists)\n",
        "    # convert tokenized documents into a document-term matrix\n",
        "    self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "    if method == 'TFIDF':\n",
        "      print('Getting vector representations for TF-IDF ...')\n",
        "      tfidf = TfidfVectorizer()\n",
        "      vec = tfidf.fit_transform(sentences)\n",
        "      print('Getting vector representations for TF-IDF. Done!')\n",
        "      return vec\n",
        "\n",
        "    elif method == 'LDA':\n",
        "      print('Getting vector representations for LDA ...')\n",
        "      if not self.ldamodel:\n",
        "        self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, \n",
        "                                id2word=self.dictionary, passes=20)\n",
        "\n",
        "      def get_vec_lda(model, corpus, k):\n",
        "        \"\"\"\n",
        "        Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
        "        :return: vec_lda with dimension: (n_doc * n_topic)\n",
        "        \"\"\"\n",
        "        n_doc = len(corpus)\n",
        "        vec_lda = np.zeros((n_doc, k))\n",
        "        for i in range(n_doc):\n",
        "          # get the distribution for the i-th document in corpus\n",
        "          for topic, prob in model.get_document_topics(corpus[i]):\n",
        "            vec_lda[i, topic] = prob\n",
        "\n",
        "        return vec_lda\n",
        "\n",
        "      vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
        "      print('Getting vector representations for LDA. Done!')\n",
        "      return vec\n",
        "\n",
        "    elif method == 'BERT':\n",
        "      print('Getting vector representations for BERT ...')\n",
        "      from sentence_transformers import SentenceTransformer\n",
        "      model = SentenceTransformer('bert-base-nli-max-tokens')\n",
        "      vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
        "      print('Getting vector representations for BERT. Done!')\n",
        "      return vec\n",
        "\n",
        "            \n",
        "    elif method == 'LDA_BERT':\n",
        "    #else:\n",
        "      vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
        "      # print(\"vec_lda: \",vec_lda.shape) # (35116, 15)\n",
        "      vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
        "      # print(\"vec_bert: \",vec_bert.shape) # (35116, 768)\n",
        "      vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
        "      self.vec['LDA_BERT_FULL'] = vec_ldabert #[35116,783]\n",
        "      if not self.AE:\n",
        "        # self.AE = Autoencoder()\n",
        "        self.AE = NTM_Autoencoder()\n",
        "        print('Fitting Autoencoder ...')\n",
        "        self.AE.fit(vec_ldabert)\n",
        "        print('Fitting Autoencoder Done!')\n",
        "      # vec = self.AE.encoder.predict(vec_ldabert)\n",
        "      # return vec\n",
        "      z_mean, _, _ = self.AE.encoder.predict(vec_ldabert)\n",
        "      return z_mean\n",
        "\n",
        "  def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
        "    \"\"\"\n",
        "    Fit the topic model for selected method given the preprocessed data\n",
        "    :docs: list of documents, each doc is preprocessed as tokens\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # Default method\n",
        "    if method is None:\n",
        "      method = self.method\n",
        "    # Default clustering method\n",
        "    if m_clustering is None:\n",
        "      m_clustering = KMeans\n",
        "\n",
        "    # turn tokenized documents into a id <-> term dictionary\n",
        "    if not self.dictionary:\n",
        "      self.dictionary = corpora.Dictionary(token_lists)\n",
        "      # convert tokenized documents into a document-term matrix\n",
        "      self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "\n",
        "    ####################################################\n",
        "    #### Getting ldamodel or vector representations ####\n",
        "    ####################################################\n",
        "\n",
        "    if method == 'LDA':\n",
        "      if not self.ldamodel:\n",
        "        print('Fitting LDA ...')\n",
        "        self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, \n",
        "                                 id2word=self.dictionary, passes=20)\n",
        "        print('Fitting LDA Done!')\n",
        "    else:\n",
        "      print('Clustering embeddings ...')\n",
        "      self.cluster_model = m_clustering(self.k)\n",
        "      self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
        "\n",
        "      self.cluster_model.fit(self.vec[method])\n",
        "      print('Clustering embeddings. Done!')\n",
        "\n",
        "  def predict(self, sentences, token_lists, out_of_sample=None):\n",
        "    \"\"\"\n",
        "    Predict topics for new_documents\n",
        "    \"\"\"\n",
        "    # Default as False\n",
        "    out_of_sample = out_of_sample is not None\n",
        "\n",
        "    if out_of_sample:\n",
        "      corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
        "      if self.method != 'LDA':\n",
        "        vec = self.vectorize(sentences, token_lists)\n",
        "        print(vec)\n",
        "    else:\n",
        "      corpus = self.corpus\n",
        "      vec = self.vec.get(self.method, None)\n",
        "\n",
        "    if self.method == \"LDA\":\n",
        "      lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
        "                            key=lambda x: x[1], reverse=True)[0][0], corpus)))\n",
        "    else:\n",
        "      lbs = self.cluster_model.predict(vec)\n",
        "    return lbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUMNUjs61Odp"
      },
      "outputs": [],
      "source": [
        "# path = \"/content/gdrive/MyDrive/fucking_paper/dataset/phone_segement.csv\"\n",
        "path = \"/content/gdrive/MyDrive/fucking_paper/dataset/segement.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3S29_9d-pu6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_preprocess(review_list):\n",
        "  tmp = False\n",
        "  new_array = []\n",
        "  new_review_list = []\n",
        "  for sentence in review_list:\n",
        "    sentence = sentence.lstrip().rstrip()\n",
        "    sentence = sentence.split()\n",
        "    # print(sentence)\n",
        "    if len(sentence) == 1:\n",
        "      # print(\"a\")\n",
        "      word = sentence[0]\n",
        "      tmp = True\n",
        "      new_array.append(word)\n",
        "    else:\n",
        "      if tmp == True:\n",
        "        # print(\"b\")\n",
        "        word = \" \".join(new_array)\n",
        "        new_review_list.append(word)\n",
        "        tmp = False\n",
        "        new_array = []\n",
        "      # print(\"c\")\n",
        "      word = \" \".join(sentence)\n",
        "      new_review_list.append(word)\n",
        "  return new_review_list"
      ],
      "metadata": {
        "id": "4kR_ILS7hp0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPFazO3n1IuQ",
        "outputId": "0f2c69a2-8964-4410-9a06-9ac9a0fa03e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting LDA ...\n",
            "Fitting LDA Done!\n",
            "Coherence: 0.2873957365601844\n",
            "Silhouette Score: None\n"
          ]
        }
      ],
      "source": [
        "# def main():\n",
        "# method = \"LDA_BERT\"\n",
        "method = \"LDA\"\n",
        "ntopic = 15\n",
        "cwd = os.getcwd() \n",
        "# filename = \"phone_segement.csv\"\n",
        "# txtPath = os.path.join(cwd,'dataset','cellphone',filename) \n",
        "with open(path, 'r', encoding=\"utf-8\",errors='ignore') as file:\n",
        "  meta = pd.read_csv(file)\n",
        "# print(meta.shape) #(24130, 16)\n",
        "rws = meta.segement\n",
        "\n",
        "sentences = []  # sentence level preprocessed\n",
        "token_lists = []  # word level preprocessed\n",
        "# idx_in = []  # index of sample selected\n",
        "\n",
        "for i, review in enumerate(rws):\n",
        "  review = review.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").split(\",\")\n",
        "  new_review = sentence_preprocess(review)\n",
        "  for sentence in new_review:\n",
        "    sentences.append(sentence.lstrip().rstrip())\n",
        "    # print(sentence.lstrip().rstrip())\n",
        "    token_list = word_tokenize(sentence.lstrip().rstrip())\n",
        "    token_lists.append(token_list)\n",
        "\n",
        "# # print(\"token len: \",len(token_lists)) [ [w1,w2,w3..], [w1,w2], ...]\n",
        "# print(\"sentences len: \",len(sentences)) # 35116個句子  # 275961個句子\n",
        "\n",
        "tm = Topic_Model(k = ntopic, method = method)\n",
        "# Fit the topic model by chosen method\n",
        "tm.fit(sentences, token_lists)\n",
        "\n",
        "print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
        "print('Silhouette Score:', get_silhouette(tm))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4W1XN7su6ZF"
      },
      "outputs": [],
      "source": [
        "# visualize and save img\n",
        "# visualize(tm)\n",
        "for i in range(tm.k):\n",
        "  get_wordcloud(tm, token_lists, i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEhcY4d2L00e"
      },
      "outputs": [],
      "source": [
        "# if __name__ == '__main__':\n",
        "#   main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}